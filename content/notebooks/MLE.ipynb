{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008855f-8f43-432c-a9aa-1b9e50237421",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(repr) ; options(repr.plot.width = 5, repr.plot.height = 6) # Change plot sizes (in cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdc43f-85b0-4c6b-99c9-71ec09f88263",
   "metadata": {},
   "source": [
    "# Model Fitting using Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b676e-ead2-4180-a9bf-8a2387a19622",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this Chapter we learn about and work through various examples of model fitting to biological data using Maximum Likelihood. It is recommended that you see this introductory [lecture](https://github.com/MulQuaBio/MQB/tree/main/content/lectures/EnE_Modelling_Intro) on model fitting in Ecology and Evolution. \n",
    "\n",
    "[Previously](./NLLS.ipynb), we learned how to fit a mathematical model/equation to data by using the Least Squares method (linear or nonlinear). That is, we choose the parameters of model being fitted (e.g., straight line) to minimize the sum of the squares of the residuals/errors around the fitted model. \n",
    "\n",
    "An alternative to minimizing the sum of squared errors is to find parameters to the function such that the * likelihood * of the parameters, given the data and the model, is maximized. \n",
    "\n",
    "<!-- Please see the [lectures](https://github.com/vectorbite/VBiTraining2/tree/master/lectures) for the theoretical background to the following examples.\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182cf96-7230-47aa-9173-5fe7ba63b2d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Probability review\n",
    "\n",
    "We will begin with a review of foundational probability theory.\n",
    "<!-- \n",
    "Axioms. Common discrete and continuous random variables, probability mass and density functions, cumulative functions.\n",
    "\n",
    "Expectation, variance, statistical moments, moment generating functions. \n",
    "\n",
    "Central limit theorem, Weak law of large numbers. \n",
    " -->\n",
    "\n",
    "<!-- #### Acknowledgements\n",
    "This series of five notebooks are compiled by Tin-Yu Hui based on existing materials. Special thanks to Dan Reuman who used to teach this module a long time ago. Some examples are extracted from Mick Crawley's GLM course, where I had the pleasure to attend, both as a student and as a GTA. Any errors that remain are, of course, my sole responsibility. \n",
    " -->\n",
    "\n",
    "### The three axioms of probability\n",
    "\n",
    "These axioms are the building blocks of modern theories of probability and statistics. \n",
    "\n",
    "1) For any event $A$ in a sample space $S$, $Pr(A)\\geqslant 0$.\n",
    "\n",
    "2) $Pr(S)=1$\n",
    "   \n",
    "3) For disjoint events $A_1, A_2, A_3, ...$, then $Pr(A_1\\cup A_2\\cup A_3\\cup ...)=Pr(A_1)+Pr(A_2)+Pr(A_3)+...$\n",
    "\n",
    "We assign a probability measure $Pr(A)$ to an event $A$. The first axiom states that probability is always non-negative. The smallest probability is zero (i.e. impossible). The second axiom states that the probability of the whole sample space is one. The sample space $S$ contains all possible outcomes for the given random experiment. This also specifies the upper bound for a probability. For the third axiom, the probability of the union of disjoint (i.e. non-overlapping) events equals the sum of their individual probabilities. Think of a Venn diagram. \n",
    "\n",
    "### Random variables\n",
    "A random variable (r.v.) is a variable that takes on its value by chance. A r.v. can take on a set of possible values, each with an associated probability. To fully characterise a r.v. we need to know 1) all its possible outcomes, which form the domain or support of the r.v., and 2) the probability of getting each outcome. \n",
    "\n",
    "Example: Let $X$ be the outcome of a coin toss. Certainly, $X$ is random. There can only be two possible outcomes: head or tail. If the coin is fair, then $Pr(X=head)=Pr(X=tail)=0.5$. These statements jointly characterise the r.v. $X$. \n",
    "\n",
    "#### Discrete random variables\n",
    "Some r.v. take a discrete collection of values. We call them discrete r.v.. An example of a discrete r.v. is the outcome from rolling a fair die. \n",
    "\n",
    "A probability *mass* function (pmf) for a discrete r.v. $X$ is a function that describes the relative probability that $X$ takes each of its possible values. In most textbooks, the pmf is written as $f(x)$ or $f_X(x)$. See #2. for more notations. \n",
    "\n",
    "##### Bernoulli random variable\n",
    "A Bernoulli r.v. is the simplest r.v. with two outcomes: success (1) or failure (0). It has one parameter $p$, the probability of success, which is bounded between 0 and 1. If $X\\sim Bernoulli(p)$ then it is obvious that $Pr(X=1)=p$ and $Pr(X=0)=1-p$. While these two equations technically summarise the probabilities, the pmf has an alternative expression: $f_X(x)=p^x(1-p)^{1-x}$. \n",
    "\n",
    "Note that $f_X(x)=0$ elsewhere (outside of the support), but this statement is often too trivial to be included. \n",
    "\n",
    "##### Binomial random variable\n",
    "A binomial r.v. is the sum of $n$ independent and identically distributed (i.i.d.) Bernoulli r.v. hence it takes values on $\\{0, 1, 2, ..., n\\}$. It is a two-parameter r.v.: $p$ the probability of success, inherited from Bernoulli r.v., and $n$ the number of i.i.d. Bernoulli trials. If $X\\sim binomial(n, p)$ then its pmf is\n",
    "$$f_X(x)=C^n_{x}p^x(1-p)^{n-x}$$\n",
    "where $C^n_{x}$ is the number of combinations when we choose $x$ objects from $n$. Order of selection does not matter here. \n",
    "\n",
    "##### Poisson random variable\n",
    "A Poisson r.v. models the number of events occurring in a fixed interval of time. Since it is a count, its possible values are all non-negative integers $\\{0, 1, 2, 3, ...\\}$. While there are infinitely many possible outcomes it is still regarded as a discrete r.v.. \n",
    "\n",
    "Poisson has one parameter which is the rate of occurrance $\\lambda>0$. If $X\\sim Poisson(\\lambda)$ then\n",
    "$$f_X(x)=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}$$\n",
    "\n",
    "If $X\\sim binomial(n, p)$ with reasonably large $p$ and reasonably small $np$, then $X$ can be approximated by a Poisson r.v. with $\\lambda=np$. That is, the number of rare events can be modelled by Poisson. \n",
    "\n",
    "####  Continuous random variables\n",
    "Continuous r.v., in contrast, take a whole range of real-number values (think of tomorrow's temperature or allele frequencies). To accommodate continuous r.v.., a probability *density* function (pdf) is in place to describe the relative probability that the r.v. takes each value in the range of possible values. \n",
    "\n",
    "Recall: The range of possible values within non-zero probability is called the *support* of a r.v.. \n",
    "\n",
    "#####  Uniform random variable\n",
    "A uniform r.v. is a continous r.v. with two parameters $a$ and $b$, which are the lower and upper bounds (support). If $X\\sim U(a,b)$ then\n",
    "$$f_X(x)=1/(b-a)$$\n",
    "which looks like a horizontal line from $a$ to $b$. \n",
    "\n",
    "##### Exponential random variable\n",
    "Exponential r.v. models the time between two successive events (remember Poisson r.v.?). Since it is a measure of time, it is continous with support $[0, \\infty)$ (inclusive of 0, but always smaller than infinity). A one-parameter r.v. which shares the same rate paramter $\\lambda$ with Poisson. If $X\\sim Exponential(\\lambda)$ then\n",
    "$$f_X(x)=\\lambda e^{-\\lambda x}$$\n",
    "\n",
    "#####  Normal random variable\n",
    "Later, we will learn why normal is the most famous r.v. of all and why normal approximation usually holds even if we have limited knowledge of the underlying distribution. $X\\sim N(\\mu, \\sigma^2)$, it takes values over the entire real number line, from negative to positive infinity, or $x \\in \\Re$. Although its bell-shaped pdf is widely known and aesthetically pleasing, its mathematical form is not as memorable: \n",
    "$$f_X(x)=\\frac{1}{\\sqrt{2\\pi \\sigma}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "It is a two-parameter r.v. with $\\mu$ and $\\sigma^2$ if you have not already realised. \n",
    "\n",
    "####  Some notations\n",
    "Most textbooks use the function $f()$ to denote a pmf/pdf. Some may specify the r.v. of interest through the subscript, e.g. $f_X()$. Capital letters are understood to be used for r.v. . The use of subscripts is extremely helpful when multiple r.v. are involved, say, when mentioning $X$ and $Y$ and their associated $f_X(x)$ and $f_Y(y)$. The lowercase $x$ inside the round brackets indicates the value at which the pmf/pdf is evaluated. These small $x$ or $y$ are real numbers (not r.v.). Numbers are numbers, r.v. are r.v.. \n",
    "\n",
    "Some texts may even state the associated parameter(s) $\\theta$ in the pmf/pdf, say, $f(x; \\theta)$ or $f(x|\\theta)$. The latter reads as \"$x$ given $\\theta$\". \n",
    "\n",
    "####  Built-in statistical tables in R\n",
    "We should always make good use of the built-in statistical functions in R. For example, there are <code>pnorm()</code>, <code>dnorm</code>, <code>qnorm()</code>, and <code>rnorm()</code> for normal distribution. The prefix <code>p</code> returns the cmf/cdf (see #3.2 below), <code>d</code> for the pmf/pdf, <code>q</code> for quantiles (for hypothesis testing), and <code>r</code> for random number generation. We will experiment some of these functions in today's practical. \n",
    "\n",
    "### Probability and cumulative functions\n",
    "####  Properties of probability mass and density functions\n",
    "Per discussed, pmf/pdf are functions to describe the relative probabilities of the outcomes and to characterise a r.v.. From the first axiom, probabilities are non-negative, hence the pmf/pdf never go below the horizontal axis. From the second axiom, we learnt that the sum of pmf bars must be one:\n",
    "$$\\sum_{all~possible~outcomes} f_X(x)=1$$\n",
    "For continuous case, if we take the limit of summation (of vertical bars) it becomes integration: \n",
    "$$\\int_{all~possible~outcomes} f_X(x)dx=1$$\n",
    "That is, the *area* under a pdf must be one. \n",
    "\n",
    "####  Cumulative mass and density functions\n",
    "We use capital $F()$ for cumulative functions (cmf/cdf). As its name suggests, $F_X(x)=Pr(X\\leqslant x)$ by definition. It is a non-decreasing function with $F(-\\infty)=0$ and $F(\\infty)=1$. For discrete r.v., \n",
    "$$F_X(x)=\\sum_{x_i\\leqslant x}f_X(x_i)$$\n",
    "For continuous case, $F_X(x)$ is the area under the pdf curve, from $-\\infty$ to $x$: \n",
    "$$F_X(x)=\\int_{-\\infty}^{x}f_X(t)dt$$\n",
    "I hope you still remember the fundamental theorem of calculus. Conversely, we can obtain pdf by differentiating cdf. You only need either the cumulative or probability function to characterise a r.v. \n",
    "\n",
    "### Statistical moments and expectation\n",
    "####  Expectation\n",
    "Imagine an experiment that can be repeated for *infinitely* many times. Imagine you keep tossing a coin or keep drawing random numbers from a given distribution for *infinitely* many times. The expecation is the \"average\" of the said experiment. \n",
    "\n",
    "Of course, this \"average\" is hypthetical one as nobody can afford having *infinitely* many repeats. Here we describe the \"average\" behaviour of a r.v. on the population level. Try not to confuse with the \"sample average\" that we tend to calculate from real data. In fact, today's discussion does not involve any data. We are merely discussing the characteristics of r.v. based on some given random mechanisms. \n",
    "\n",
    "For discrete r.v., \n",
    "$$E[X]=\\sum_{all~possible~outcomes}xf(x)$$\n",
    "For continuous r.v., \n",
    "$$E[X]=\\int_{-\\infty}^{+\\infty}xf(x)dx$$\n",
    "You can replace the bounds of the integral by the support of $X$. $E[X]$ is the expected value of $X$, the \"average\" value weighted according to the pmf/pdf. $E[X]$ is also called the population mean or true mean of the r.v. $X$. It is a measure of central tendency. \n",
    "\n",
    "####  Variance\n",
    "Similarly, we have the population variance, which is given by\n",
    "$$Var[X]=E[(X-E[X])^2]$$\n",
    "The formula above suggests that variance is the expected distance squared of the r.v. $X$ from its population mean. In practice we tend to use this alternative form: \n",
    "$$Var[X]=E[X^2]-(E[X])^2$$\n",
    "There is no surprise that variance is a measure of dispersion. \n",
    "\n",
    "####  Higher moments\n",
    "In general, the $n^{th}$ *raw* moment of $X$ is $E[X^n]$: \n",
    "$$E[X^n]=\\int x^nf(x)dx$$\n",
    "\n",
    "And the $n^{th}$ central moment is $E[(X-E[X])^n]$. In most cases only the first few moments are studied. For example, the third moment of a r.v. describes its skewness (e.g., a normal r.v. has 0 skewness as a bell curve is symmetric about $\\mu$), and the fourth moments is a measure of kurtosis (fat tails). \n",
    "\n",
    "Note that not all distributions have finite moments. One example is the Cauchy distribution (t-distribution with 1 degree of freedom) whose $E[X]$ is undefined. \n",
    "\n",
    "####  More on the expectation operator\n",
    "We get a real number after taking the expectation from a r.v., we get a real number. Note that expectation is linear: \n",
    "$$E[aX+bY]=aE[X]+bE[Y]$$\n",
    "for any r.v. $X$, $Y$ and any real numbers $a$, $b$. \n",
    "\n",
    "In some cases, we may be required to transform a r.v. or to calculate the expectation of a transformed r.v.: \n",
    "$$E[g(X)]=\\int g(x)f(x)dx$$ for any real function $g$. \n",
    "\n",
    "Note that $g(X)$ itself is another r.v. with its own support, pdf/pmf, expecation, etc.. The same is true for $(X+Y)$, that is, the sum (or prodictof r.v. is another r.v.. Remember, transformation of a r.v. yields another r.v.. A r.v. will not suddenly turn into a real number. \n",
    "\n",
    "####  Moment generating function\n",
    "A moment-generating function (mgf) is the third way to characterise an r.v.. $M_X(t)$ is a carefully crafted function from $X$ such that it \"generates\" statistical moments through its derivatives at $t=0$, note that $t$ is a dummy variable. The $n^th$ moment of $X$ is: \n",
    "$$E[X^n]=\\frac{d^nM_X(t)}{dt^n}|_{t=0}$$\n",
    "\n",
    "For keen readers, $M_X(t)=E[e^{tX}]$. \n",
    "\n",
    "### Central limit theorem and Weak law of large numbers\n",
    "####  Central limit theorem\n",
    "Let $\\{X_1, X_2, X_3, ..., X_n\\}$ be i.i.d. r.v. with finite $E[X_i]=\\mu$ and finite $Var[X_i]=\\sigma^2$. Also let $\\bar{X_n}=(X_1+X_2+...+X_n)/n$ be the sample mean of these r.v. ($\\bar{X_n}$ is another r.v.). The central limit theorem states that as $n\\rightarrow \\infty$, the r.v. $\\sqrt{n}(\\bar{X_n}-\\mu)$ converges *in distribution* to a normal distribution: \n",
    "$$\\sqrt{n}(\\bar{X_n}-\\mu) \\xrightarrow{d} N(0,\\sigma^2)$$\n",
    "\n",
    "See today's practical for visualisation. \n",
    "\n",
    "####  Weak law of large numbers\n",
    "Let us consider a similar series of i.i.d. r.v. $\\{X_1, X_2, X_3, ..., X_n\\}$ with finite $E[X_i]=\\mu$. The weak law of large numbers states that the sample mean $\\bar{X_n}$ converges *in probability* to the expected value when $n\\rightarrow \\infty$. That is, for any postive $\\epsilon$, \n",
    "$$\\lim_{n\\rightarrow \\infty}Pr(|\\bar{X_n}-\\mu|<\\epsilon)=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35372dfd-d19b-47ac-9548-f37525bfc213",
   "metadata": {},
   "source": [
    "## Multivariate random variables, Likelihood functions\n",
    "\n",
    "<!-- We will now learn about likelihood and log-likelihood functions, Point estimation, maximisation by hand, maximisation by computer, MLE coin-tossing example, i.i.d. normal example. \n",
    " -->\n",
    "\n",
    "### Multivariate random variables\n",
    "Quite often multiple r.v. are considered simultaneously in a system due to their interaction. For example, population sizes of the species within a predator-prey system, allele frequencies among tightly linked loci, traits within an individual, and many more. It is essential for us to extend the discussions of r.v. and pmf/pdf to multivariate cases. \n",
    "\n",
    "#### Joint probability density functions\n",
    "Given a pair of r.v. $X$ and $Y$, the joint pmf/pdf is $f_{X,Y}(x,y)$, a higher-dimensional function. In this bivariate case the joint pdf can be visualised in the form of a 3D plot. Following the same rules as the univeriate case, $f_{X,Y}(x,y)$ must be non-negative, and must integrate to one: \n",
    "$$\\int\\int f_{X,Y}(x,y)dxdy=1$$\n",
    "\n",
    "##### Bivariate normal random variables\n",
    "The support of a bivariate normal r.v. is the entire real number *plane*, or $\\Re^2$. In univariate normal we have two parameters $\\mu$ and $\\sigma^2$ which are both numbers. In bivariate normal we have the multivariate analogy of the two, but this time they are called the mean vector $\\boldsymbol{\\mu}=\\begin{pmatrix}\\mu_X \\\\ \\mu_Y \\end{pmatrix}$ and variance-covariance matrix $\\boldsymbol{\\Sigma}=\\begin{bmatrix}\\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\ \\rho \\sigma_X \\sigma_Y & \\sigma_Y^2 \\end{bmatrix}$. \n",
    "\n",
    "The entries of $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ are numbers. For instance, here we have the individual means and variances for $X$ and $Y$, and also $\\rho$ for the correlation between the two. Check the link below to visualise the joint pdf of a bivariate normal r.v.. \n",
    "\n",
    "http://socr.ucla.edu/htmls/HTML5/BivariateNormal/\n",
    "\n",
    "In general, for a $k$-dimensional multivariate normal, the mean vector is of length $k$, and the variance-covariance matrix is a $k$-by-$k$ symmetric and (semi)-postive definite matrix. Note that many applications rely on the decomposition of $\\boldsymbol{\\Sigma}$, such as Principal component analysis. \n",
    "\n",
    "#### Marginal distributions\n",
    "Sometimes we are interested in one r.v. without referencing to the values of another. In this case the focus on finding the marginal distribution of the r.v. of interest. In a bivariate case with r.v. $X$ and $Y$ and their joint pdf $f_{X,Y}(x,y)$, the marginal pdf of $X$ is \n",
    "$$f_X(x)=\\int f_{X,Y}(x,y)dy$$\n",
    "The integration along the $y$ axis is to marginalise out (get rid of) the uninterested r.v. $Y$. As a result, there will be no $y$ left in $f_X(x)$. Similarly, we can integrate along the other axis to find $f_Y(y)$. Note that the marginal pdf is indeed a pdf therefore the properties we discussed yesterday remain applicable. \n",
    "\n",
    "#### Conditional distributions\n",
    "If the value of r.v. $Y$ is known (i.e. we know $Y=y$) then this may give additional information on another r.v. $X$. The distribution of interest here is the conditional distribution of $X$ given $Y$: \n",
    "$$f_{X|Y}(x|y)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}$$\n",
    "\n",
    "We can imagine $f_{X|Y}(x|y)$ as a slice of the joint pdf at $Y=y$ after normalisation. Of course we can find the conditional of $Y|X$ from the joint and the marginal of $X$. The following relationship is established: \n",
    "$$f_{X|Y}(x|y)f_Y(y)=f_{X,Y}(x,y)=f_{Y|X}(y|x)f_X(x)$$\n",
    "I hope some of you may notice that this is the Bayes' theorem. I trust that you will be exposed to the theorem and its applications next week hence I shall not go beyond this. The key here is learn how we can obtain one distribution from the others via the \"joint = marginal x conditional\" relationship. \n",
    "\n",
    "###. Expectation and covariance\n",
    "We can calculate expectations from the joint, conditional, or marginal distributions as before. With multiple r.v. come with more rules. \n",
    "\n",
    "#### Law of total expectation\n",
    "Given two r.v. $X$ and $Y$, \n",
    "$$E[Y]=E[E(Y|X)]$$\n",
    "You first get the conditional expecation before taking another (outer) expectation to obtain the marginal expecation of $Y$. \n",
    "\n",
    "#### Law of total variance\n",
    "$$Var[Y]=E[Var(Y|X)]+Var[E(Y|X)]$$\n",
    "which can be easily derived from the law of total expectation above. This is also called the \"Eve's formula\". \n",
    "\n",
    "#### Covariance and corelation\n",
    "Covariance measures the joint variability of a pair of r.v.. Given $X$ and $Y$, \n",
    "$$cov[X,Y]=E[XY]-E[X]E[Y]$$\n",
    "where $E[XY]=\\int \\int xyf_{X,Y}(x,y)dxdy$. \n",
    "\n",
    "The correlation is the normalised measure describing the *linear* association between a pair of r.v.: \n",
    "$$corr[X,Y]=\\frac{cov[X,Y]}{Var[X]Var[Y]}$$\n",
    "which is always bounded between -1 and +1. Note that only linear association is captured by correlation. A pair of r.v. can have zero correlation but are perfectly \"related\" (e.g. they go around a circle). \n",
    "\n",
    "Yesterday we mentioned that expectation is linear, that is, $E[X+Y]=E[X]+E[Y]$. This is true regardless of their correlation. The same cannot be said for variance:\n",
    "$$Var[aX+bY]=a^2Var[X]+b^2Var[Y]+2abcov[X,Y]$$\n",
    "for some real numbers $a$ and $b$. \n",
    "\n",
    "###. Independence\n",
    "Independence is the strongest assumption in statistics as there is no way to test for it in real world. How can one confirm the toss of the first coin has nothing to do with the second? How can one be so certain that my action here will have zero influence to that by another person 1000 km away? Two events are independent if the occurrence of one does not affect the occurrence of another. \n",
    "\n",
    "If $X$ and $Y$ are independent then $corr[X,Y]=0$, but the reverse in not always true. In general $corr[X,Y]=0$ does not imply independence expect for some known cases (e.g. in multivariate normal). In fact, the assumption of independence is so strong that it guarantees $corr[g(x), h(y)]=0$ for all real functions $g$ and $h$ you can possibly imagine. \n",
    "\n",
    "#### Joint probability density functions under independence\n",
    "$X$ and $Y$ are independent if and only if their joint pdf is the product of their marginals: \n",
    "$$f_{X,Y}(x,y)=f_X(x)f_Y(y)$$\n",
    "\n",
    "### Interval\n",
    "So far we have been discussing the properties of r.v., calculating probabilities and expectations etc. based on some given assumptions and fixed parameter value. A typical question would involve calculating the probability that 0, 1, 2 buses will arrive in the next unit of time. For example, if we assume the arrival of buses $X$ follow a Poisson distribution with $\\lambda=3$, then\n",
    "$$f_X(x, \\lambda=3)=\\frac{\\lambda^xe^{-\\lambda}}{x!}=\\frac{3^xe^{-3}}{x!}$$\n",
    "is a function of $x$ and a valid pmf. With known $/lambda$, we can calculate those probabilities by substituting $x=0$, $x=1$, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818507eb-b962-4625-8486-b4f7495dce7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEX9/v0AAABMTUxnaGd7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\nrkJggg==",
      "text/plain": [
       "Plot with title \"pmf of Poisson with lambda=3\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "probability<-dpois(0:15, lambda=3)\n",
    "barplot(probability, space=1, beside=F, xlab='x', ylab='probability', main='pmf of Poisson with lambda=3')\n",
    "axis(1, at=seq(1, 31, 2)+0.5, labels=0:15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822f4c9-ed81-4378-8de3-d189620f914a",
   "metadata": {},
   "source": [
    "From the pmf plot we know that $Pr(X=0)=0.0497$ and $Pr(X=1)=0.149$, and so on. Thur far there is no observation. We are still working out the probabilities of some potential outcomes inside our brain. We are still waiting for our bus. \n",
    "\n",
    "Now, the tide has turned that we actually have some data! Say we observed 3 buses within a unit time. This time we have $x=3$, but $\\lambda$ is unknown. From the same Poisson \"function\", \n",
    "$$f_X(x=3, \\lambda)=\\frac{\\lambda^xe^{-\\lambda}}{x!}=\\frac{\\lambda^3e^{-\\lambda}}{3!}$$\n",
    "it becomes a function of $\\lambda$ once the data is observed. One can plot this $f_X(x=3, \\lambda)$ against $\\lambda$, it is no longer a pmf (previously we plot it against $x$). By doing that, we are asking \"what is the proability of seeing 3 buses if $\\lambda=1$?\", and what if $\\lambda=1.5$? And so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da96450c-e95c-458a-a320-0c4f4d40f18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEX9/v0AAABMTUxnaGd7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\nAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"this is NOT a pdf/pmf\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(seq(0.1, 15, 0.01), dpois(3, lambda=seq(0.1, 15, 0.01)), type='l', lwd=2, \n",
    "     xlab='lambda', ylab='robability of seeing 3 buses at this lambda value', main='this is NOT a pdf/pmf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e356e-8af8-4e4f-aa71-8113a913acc3",
   "metadata": {},
   "source": [
    "Essentially we are making inference on the unknown parameter from our observation. In the remaining sections of this notebook, we will formally introduce the likelihood framework for statistical inference. Welcome to the world of Statistics. \n",
    "\n",
    "###. Maximum Likelihood Estimation\n",
    "Many said Maximum Likelihood Estimation (MLE) was invented by Sir Ronald Fisher, but I (as a statistician and geneticist many degrees inferior to Fisher) argue that such idea needs no invention but rather has been embedded in everybody's mind since the beginning of civilisation. I sincerely hope that by the end of this week you will appreciate MLE as a collection of methods who share a common belief towards how the \"best parameters\" should behave. \n",
    "\n",
    "A distribution of a r.v., or a statistical model comes with parameters. When a *dataset* is collected, and a statistical *model* is proposed, MLE provides a set of rules to find the best estimates of the associated *parameters*. In words, MLE aims to find the parameter values that make the observed dataset most \"probable\". \n",
    "\n",
    "The triplets: data, model, parameters. \n",
    "\n",
    "#### Likelihood function\n",
    "To quantify how \"probable\" a paramter value is in producing our observed data we introduce the likelihood function $L(\\underline{\\theta})$. From now on we use $\\underline{\\theta}$ to denote a vector of parameters simply for generalisation (and similarly $\\underline{x}$ for a vector of data). \n",
    "\n",
    "By definition, the likelihood function is the joint density (pdf) of our observation $\\underline{x}$:\n",
    "$$L(\\underline{\\theta})=L(\\underline{\\theta}|\\underline{x})=f(\\underline{x}|\\underline{\\theta})=f(x_1, x_2, ..., x_n|\\underline{\\theta})$$\n",
    "\n",
    "Once $\\underline{x}$ is observed, you may imagine that $\\underline{x}$ turn into real numbers, and $L(\\underline{\\theta}|\\underline{x})$ is a function of $\\underline{\\theta}$ only. We can evaluate $L(\\underline{\\theta}|\\underline{x})$ along $\\underline{\\theta}$, and find a $\\underline{\\hat{\\theta}}$ that maximises the function $L(\\underline{\\theta}|\\underline{x})$. This is the spirit of MLE. \n",
    "\n",
    "#### Independent observations and log-likelihood\n",
    "The likelihood function is the joint pdf of our observations. If they are independent, then the joint pdf becomes the product of univariate pdfs: \n",
    "$$L(\\underline{\\theta}|\\underline{x})=f(x_1, x_2, ..., x_n|\\underline{\\theta})$$\n",
    "$$=f_{X_1}(x_1)f_{X_2}(x_2)...f_{X_n}(x_n)$$\n",
    "which streamlines the construction of the likelihood function. \n",
    "\n",
    "Besides, many prefers to work on the log-likelihood function $l(\\underline{\\theta}|\\underline{x})=\\ln(L(\\underline{\\theta}|\\underline{x}))$, as the product of pdfs immediately becomes the sum of log-pdfs. Both likelihood and log-likelihood functions attain their maximums at the same $\\underline{\\hat{\\theta}}$, because log is a monotonoic monotonic (also non-decreasing) function. \n",
    "\n",
    "In MLE, we treat parameters as fixed but unknown quantities to be estimated. We are inferring parameters, not the probability of hitting a particular outcome. The likelihood function is not a pdf hence does not integrate to one. It is a function of the parameters. \n",
    "\n",
    "#### Coin tossing example (in R)\n",
    "Assume we obtain $y$ heads from $n$ independent tosses. By definition, the likelihood function is the joint pmf of these $n$ tosses, which looks like a binomial distribution: \n",
    "$$L(p|y)=C_y^np^y(1-p)^{n-y}$$\n",
    "\n",
    "After the experiment we observed 7 heads from 10 tosses. Let us put $y=7$ and $n=3$ into the likelihood function:\n",
    "$$L(p)=C^{10}_7p^7(1-p)^3$$\n",
    "We can rewrite this function in R. I cannot stress enough that the likelihood is a function of the parameter, as seen from <code><-function(p)</code>. We then plot the likelihood function against a range of values of $p$. Pay attention to the parameter space: $p$ is from Bernoulli hence it has to be bounded between 0 and 1. The second plot shows the log-likelihood function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994a137f-b94c-437a-ae47-4b79cc77916c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "8.748e-06"
      ],
      "text/latex": [
       "8.748e-06"
      ],
      "text/markdown": [
       "8.748e-06"
      ],
      "text/plain": [
       "[1] 8.748e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEX9/v0AAABMTUxnaGd7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\nEEBIgABCAgQQEiCAkAAB/wMpsHzysyTk7wAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WRITE DOWN THE LIKELIHOOD FUNCTION\n",
    "binomial.likelihood<-function(p){\n",
    "choose(10,7)*p^7*(1-p)^3\n",
    "}\n",
    "# LET US CALCULATE THE LIKELIHOOD VALUE AT p=0.1\n",
    "binomial.likelihood(p=0.1)\n",
    "# YOU GOT SOMETHING AROUND 8.748e-06, RIGHT?\n",
    "# PLOT THE LIKELIHOOD FUNCTION FOR A RANGE OF p\n",
    "p<-seq(0, 1, 0.01)\n",
    "likelihood.values<-binomial.likelihood(p)\n",
    "plot(p, likelihood.values, ylab='likelihood', type='l', lwd=2)\n",
    "abline(v=0.7, col='red', lty=2, lwd=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34724b80-d6c3-4376-bb1c-7782fbdc8b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEX9/v0AAABMTUxnaGd7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\nRK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOG-LIKELIHOOD FUNCTION\n",
    "log.binomial.likelihood<-function(p){\n",
    "log(binomial.likelihood(p=p))\n",
    "}\n",
    "# PLOT THE LOG-LIKELIHOOD\n",
    "p<-seq(0, 1, 0.01)\n",
    "log.likelihood.values<-log.binomial.likelihood(p)\n",
    "plot(p, log.likelihood.values, ylab='log-likelihood', type='l', lwd=2)\n",
    "abline(v=0.7, col='red', lty=2, lwd=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fcd2b-1227-4898-872c-0ee28439a2f0",
   "metadata": {},
   "source": [
    "The next step is to find value of $p$ such that $L(p)$ is maximised. It appears that the peak is at 0.7, and we say $\\hat{p}=0.7$ is our maximum likelihood estimate. \n",
    "\n",
    "Beyond eyeballing, we can use R's optimisation routines to find the maixmum/minimum of a function. For univariate (one-parameter) case, we can use <code>optimize()</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8fa1a5d-53f8-4996-99d7-4b14eb7580b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$maximum</dt>\n",
       "\t\t<dd>0.699984294307121</dd>\n",
       "\t<dt>$objective</dt>\n",
       "\t\t<dd>0.266827930432933</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$maximum] 0.699984294307121\n",
       "\\item[\\$objective] 0.266827930432933\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$maximum\n",
       ":   0.699984294307121\n",
       "$objective\n",
       ":   0.266827930432933\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$maximum\n",
       "[1] 0.6999843\n",
       "\n",
       "$objective\n",
       "[1] 0.2668279\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimize(binomial.likelihood, interval=c(0,1), maximum=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb8208-1f77-49e6-ab0d-37125eb0f5d1",
   "metadata": {},
   "source": [
    "<code>\\$optimize()</code> returns the parameter value at which the function attains its maximum, where the <code>\\$objective</code> is the maximised function value. \n",
    "\n",
    "#### Coin tossing example (by hand)\n",
    "For a more general case with $y$ heads from $n$ independent tosses the likelihood function is:\n",
    "$$L(p)=C_y^np^y(1-p)^{n-y}$$\n",
    "To maximise a function means to find the first derivative, then find the point with zero slope. Of course there are other conditions to meet (e.g. the second derivative) for the global maximum. Here, let us work on the log-likelihood:\n",
    "$$l(p)=\\ln(L(p))=\\ln(C_y^n)+y\\ln(p)+(n-y)\\ln(1-p)$$\n",
    "Next, we differentiate $l(p)$ *with respect to $p$*. Note that the first term $\\ln(C_y^n)$ does not contain $p$ thus is treated as a constant:\n",
    "$$l'(p)=0+y\\frac{1}{p}+(n-y)\\frac{-1}{1-p}$$\n",
    "Next, we find a $\\hat{p}$ such that $l'(\\hat{p})=0$:\n",
    "$$\\hat{p}=\\frac{y}{n}$$\n",
    "and we say this is the MLE for the (fixed but unknown) parameter $p$. \n",
    "\n",
    "#### i.i.d. Normal samples with unknown $\\mu$\n",
    "Suppose we have $X_1, X_2, ..., X_n$ i.i.d. samples from $N(\\mu, 1)$. Variance is known but $\\mu$ is the parameter to be estimated. By definition, the likelihood is a function of $\\mu$, and is the joint pdf of our samples:\n",
    "$$L(\\mu)=f(x_1, x_2, ..., x_n)$$\n",
    "Here $f$ is a joint (higher-dimensional) pdf. As these samples are independent, the joint pdf becomes the product of individual pdfs:\n",
    "$$L(\\mu)=f_{X_1}(x_1)f_{X_2}(x_2)...f_{X_n}(x_n)$$\n",
    "Note that $f_{X_i}$ are individual (one-dimensional) pdfs. Next, we make use of the fact that they are i.i.d. normal: \n",
    "$$L(\\mu)=\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{(x_i-\\mu)^2}{2}})$$\n",
    "$$=(\\frac{1}{\\sqrt{2\\pi}})^ne^{\\frac{1}{2}\\sum_{i=1}^n(x_i-\\mu)^2}$$\n",
    "The log-likelihood function is:\n",
    "$$l(\\mu)=constant-\\frac{1}{2}\\sum_{i=1}^n(x_i-\\mu)^2$$\n",
    "Next, differentiate it w.r.t. $\\mu$:\n",
    "$$l'(\\mu)=0-\\frac{1}{2}[-2\\sum_{i=1}^n(x_i-\\mu)]$$\n",
    "Find a value of $\\hat{\\mu}$ such that $l'(\\hat{\\mu})=0$. That is to solve the following equation:\n",
    "$$\\sum_{i=1}^n(x_i-\\hat{\\mu})=0$$\n",
    "Rearranging the terms gives:\n",
    "$$\\hat{\\mu}=\\frac{\\sum_{i=1}^nx_i}{n}$$\n",
    "And this is our MLE. \n",
    "\n",
    "#### i.i.d. Normal samples unknown $\\mu$ and $\\sigma^2$\n",
    "With unknown $\\sigma^2$ the formulation of the log-likelihood function is the same as in the previous example but it is now a bivariate (two-paramter) function $l(\\mu, \\sigma^2)$. Because of this, the *partial* derivavtives $\\frac{\\partial l}{\\partial\\mu}$ and $\\frac{\\partial l}{\\partial\\sigma^2}$ need to be evaluated. A partial derivative is the derivative of a multivariate function with respect to one of the variables while treating the others as constant. At last, we need to find a pair of $(\\hat{\\mu}, \\hat{\\sigma^2})$ such that the partial derivatives are zero simultaneously. Again pay attention to the parameter space: $\\mu$ can be any real number, but $\\sigma^2$ (or $\\sigma$ itself) must be non-negative. I leave this as an exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5717bf0-8fad-48c7-be20-7790b1739144",
   "metadata": {},
   "source": [
    "## Theoretical guarantees\n",
    "\n",
    "<!-- More MLE examples: linear regression, general linearised regression (logistic and Poisson). \n",
    "\n",
    "Using <code>optim()</code>. \n",
    "\n",
    "Properties of ML estimators: asymptotically unbiased, asymptotically efficient, consistent, asymptotically normal, invariance. \n",
    " -->\n",
    "\n",
    "### More MLE examples\n",
    "\n",
    "#### Linear regression\n",
    "I hope we are all familiar with the simplest form of a linear regression. It comprises a vector of response $\\underline{x}$ and a vector of independent (or explanatory) variable $\\underline{x}$: \n",
    "$$y_i=a+bx_i+\\epsilon_i$$\n",
    "where $a$ is the intercept and $b$ is the slope. $i=1, 2, ..., n$. $\\epsilon_i$ is the error term, and in simple linear regression, $\\epsilon_i$ are assumed to be i.i.d. $N(0,\\sigma^2)$. \n",
    "\n",
    "Now we have identified the triplets required for parameter estimation: Our data $\\underline{x}$ and $\\underline{y}$. The three parameters of interest: $a$, $b$, and $\\sigma^2$. And finally the model, which is i.i.d. normal errors. If we rearrange the regression equation such that the error is the subject:\n",
    "$$\\epsilon_i=y_i-a-bx_i$$\n",
    "Then the likelihood becomes\n",
    "$$L(\\underline{\\theta})=f(\\epsilon_1\\epsilon_2...\\epsilon_n)=\\prod_{i=1}^nf(\\epsilon_i)$$\n",
    "The first $f$ is the joint density of all the error terms, and the second $f$ is the pdf of an individual error (which is normally distributed with a common variance). Now the problem has reverted back to the one we saw in #4.6 yesterday, that we need to find a set of $(\\hat{a}, \\hat{b}, \\hat{\\sigma^2})$ such that the likelihood is maximised. I probably would delegate this task to R, see today's practical on <code>recapture.csv</code>, also available in a separate notebook. In R, we can use the built-in <code>dnorm(y-a-b*x, mean=0, sd=sigma)</code> instead of writing the normal pdf explicitly. \n",
    "\n",
    "#### Logistic regression\n",
    "Logistic regression belongs to the family of generalised linear models (GLM). It aims to relate a binary response to a set of explanatory variable(s). For example, in public health a typical binary response will be the state of the patients (dead/alive), or whether a parasite is absent or present in one's body. When I think of a binary variable, I immediately recall the Bernoulli r.v., whose outcome is either 0 or 1. In fact, logistic regression assumes that each binary outcome is a Bernoulli r.v. with its own $p_i$:\n",
    "$$Y_i\\sim Bernoulli(p_i)$$\n",
    "$i=1, 2, ..., n$. In addition, $p_i$ is influenced by the explanatory variable $x_i$. As a result, some will have a higher probability of success, depending on its relationship with $x_i$. Here I suggest the following form:\n",
    "$$p_i=\\eta^{-1}(a+bx_i)$$\n",
    "$a$ and $b$ are the coefficients, similar to the intercept and slope in linear regression. $(a+bx_i)$ is called the linear predictor. The remaining question is, what is $\\eta^{-1}$? \n",
    "$$\\eta^{-1}=\\frac{e^{a+bx_i}}{1+e^{a+bx_i}}$$\n",
    "We know from the Bernoulli r.v. that $p_i$ has to be bounded between 0 and 1, but $(a+bx_i)$ can be any real number (linear). That is why we call upon $\\eta^{-1}$ to map a real number to $[0, 1]$. $\\eta^{-1}$ is called the *expit* transformation, and its inverse $\\eta$ is *logit*, hence the name of the model. \n",
    "\n",
    "With the triplets identified we can proceed to find the ML estimates for $a$ and $b$. See <code>flowering.txt</code>, also available in a separate notebook. \n",
    "\n",
    "#### Poisson regression\n",
    "Poisson regression is another GLM which is often used when the response are counts (e.g. number of mutations/crossovers). As its name suggests, it assumes each response follows a Poisson distribution with its own rate parameter $\\lambda_i$:\n",
    "$$Y_i\\sim Poisson(\\lambda_i)$$\n",
    "$i=1, 2, ..., n$. Similar to the logistic case, we need a function to link up $\\lambda_i$ with the linear predictor. The constraint here is that $\\lambda_i$ must be non-negative due to it being a rate. An appropriate function will be exponential: \n",
    "$$\\lambda_i=e^{a+bx_i}$$\n",
    "Note that the inverse of exponential is log. \n",
    "\n",
    "In GLM, a link function provides the relationship between the lienar predictor and the mean of the distribution function. Logit and log are link functions in logistic and Poisson GLMs, respectively. I hope these examples help demystify GLMs and their model fitting via <code>glm(y~x, family=)</code>. \n",
    "\n",
    "###. <code>optim()</code>\n",
    "<code>optim()</code> is a generic optimisation routines for multivariate functions, similar to what <code>optimize()</code> does in univariate case. Here multivariate refers to the number of parameters rather than observations. It is a so powerful that its help file <code>?optim</code> provides more questions than answers. \n",
    "\n",
    "#### Inputs\n",
    "It takes quite a few arguments, usually in the following order:\n",
    "1) The mandatory <code>par=</code> vector specifies the initial condition for the search. If it is a $k$-dimensional function then a vector of length $k$ should be supplied. Avoid starting near the boundary of the parameter space. \n",
    "2) Put the function name you wish to optimise under <code>fn=</code>. Mandatory.\n",
    "3) <code>method=</code> instructs the optimisation algorithm to be used. The default is Nelder-Mead. The choice of algorithm can affect performance and numerical results. Another population option is <code>method='L-BFGS-B'</code> which takes a box-like constraint (see below).\n",
    "4) The default algorithm imposes no constraints on the parameter space which is a double-edged sword. When <code>method='L-BFGS-B'</code> is chosen, then you must supply <code>lower=</code> and <code>upper=</code> as two vectors (with lengths equal to the number of parameters). This algorithm is useful when some parameters cannot go beyond a certain value (e.g. Poisson rates $\\lambda_i$ must be non-negative). \n",
    "5) Other control options, such as tolerance, can be supplied to the control list <code>control=list((fnscale=-1))</code>. In particular, <code>fnscale=-1</code> means to maximise. The default is minimise. \n",
    "6) <code>hessian=T</code> to return the Hessian matrix. Optional but useful. \n",
    "\n",
    "#### Outputs\n",
    "It returns a big list: \n",
    "1) <code>\\$par</code> is the parameter values that maximise the function. \n",
    "2) <code>\\$value</code> returns the maximised function value.\n",
    "3) The rest are for performance and convergence checking. If you have asked for the Hessian matrix, you may find it in the output as well. \n",
    "\n",
    "There are other alternatives to <code>optim()</code>, such as <code>nlm()</code> and <code>nlminb</code>. External packages are available as well. \n",
    "\n",
    "###. Properties of ML estimators\n",
    "Below are the theoretical guarantees of ML estimators. They also justify why we are spending one whole week on likelihood. \n",
    "\n",
    "#### ML estimators are asymptotically unbiased\n",
    "If we repeat the experiment (and ML estimation) infinitely many times and obtain many $\\hat{\\theta}$, then the hypothetical average of this $ \\ hat {\\theta}$ is $\\theta$. When we talk of a hypothetical average we mean expectation: \n",
    "$$E[\\hat{\\theta}]\\rightarrow \\theta$$\n",
    "when $n\\rightarrow \\infty$. \n",
    "\n",
    "#### ML estimators are asymptotically efficient\n",
    "Efficiency means the ML estimator $\\hat{\\theta}$ usually has a lower variance compared to the other estimators (what are the other estimators?). Since it has better use of the data, it produces narrower confidence intervals (C.I.). In particular, $Var[\\hat{\\theta}]$ reaches the theoretical lower bound when $n\\rightarrow \\infty$. \n",
    "\n",
    "#### ML estimators are consistent\n",
    "The ML estimator $\\hat{\\theta}$ converges *in probability* to the true parameter $\\theta$ when $n\\rightarrow \\infty$. \n",
    "\n",
    "#### ML estimators are asymptotically normal\n",
    "The ML estimator $\\hat{\\theta}$ is asymptotically distributed as normal with mean equals the true parameter $\\theta$. This is closely related to the central limit theorem, and this explains why the magic number 1.96 works in constructing C.I.. Have you ever wondered why a z-test is used in a <code>glm()</code>?\n",
    "\n",
    "#### The invariant principle of ML estimators\n",
    "If $\\hat{\\theta}$ is the ML estimator for the parameter $\\theta$, then $g(\\hat{\\theta})$ is the ML estimator for $g(\\theta)$. Essentially we do not need to recalculate the ML estimator if the transformed parameter is of concern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e66dc7-a0e5-4c56-a2b7-59719b47bff4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Readings and Resources <a id='Readings-MLE'></a>\n",
    "\n",
    " * Bolker, B. Ecological models and data in R. (Princeton University Press, 2008).\n",
    "\n",
    "* Millar, *Maximum Likelihood Estimation and Inference*\n",
    "   \n",
    "* Crawley, *The R Book*\n",
    "\n",
    "3) Hogg & Tanis, *Probability and Statistical Inference*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
